{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5223227a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8453944f",
   "metadata": {},
   "source": [
    "# Data Science\n",
    "### 📚 Objetivo General\n",
    "Formar profesionales capaces de adquirir, procesar, analizar y comunicar información basada en datos mediante herramientas de programación, estadística y machine learning, integrando principios éticos y buenas prácticas de ciencia de datos.\n",
    "\n",
    "---\n",
    "\n",
    "## 📓 Módulo 1: Introducción al Data Science\n",
    "**Objetivo del módulo:** Comprender el rol del científico de datos y el ciclo de vida de los proyectos de data science.\n",
    "\n",
    "**Resultados de Aprendizaje:**\n",
    "- Entender las etapas de un proyecto de ciencia de datos.\n",
    "- Reconocer el valor de los datos y su impacto en diversas industrias.\n",
    "\n",
    "**Contenidos:**\n",
    "1. Historia y definición de Data Science\n",
    "2. Conceptos fundamentales y tipos de análisis\n",
    "3. Flujo de trabajo de un proyecto de datos\n",
    "4. Ciclos CRISP-DM y OSEMN\n",
    "5. Ética y privacidad en datos\n",
    "6. Herramientas y entornos comunes (Python, Jupyter, Git, VS Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72713f",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Las organizaciones de todo tipo, hace mucho tiempo que han reconocido la necesidad de almacenar datos y transformarlos en información. Esta información debe ser administrada, planificada, controlada y tratada como un activo. Este activo debe ser manipulado en forma efectiva y eficiente.\n",
    "\n",
    "La tarea de las disciplinas de Inteligencia de Negocios (Business Intelligence), Analisis de Datos (data Analytics) y Ciencia de Datos (Data Science) es tomar unos ciertos datos y transformarlos en información para describir, pronosticar y generar conocimiento a partir de ellos. Para finalmente tomar decisiones basados en esos datos.\n",
    "\n",
    "Sin embargo, para lograr estas metas se deben tener las capacidades de diseñar en forma correcta los datos a capturar para esa generación de conocimiento. ¿Se deben colectar todos los datos?, ¿cómo discriminar aquellos relevantes? ¿cómo muestrear adecuadamante si no dispongo del universo de datos? ¿cuando se debe efectuar métodos de imputación de datos?.\n",
    "\n",
    "![Pirámide de la información.](images/datoinfo0.png)\n",
    "\n",
    "## Datos\n",
    "\n",
    "Definamos datos como **Información concreta sobre hechos, elementos, etc., que permite estudiarlos, analizarlos o conocerlos**.\n",
    "\n",
    "\"los datos del censo; el análisis aportó datos de gran interés respecto a la génesis de esta fobia; cada ficha contiene los datos comerciales, fiscales y estadísticos de cada proveedor; estos datos configuran una densidad de población débil, aunque ello no descarta que haya núcleos muy poblados y muchas regiones vacías\"\n",
    "\n",
    "**Cifra, letra o palabra que se suministra a la computadora como entrada y la máquina almacena en un determinado formato**.\n",
    "\n",
    "\"al introducir palabras o números en una hoja de cálculo, la computadora los procesa y los almacena como datos en código binario\"\n",
    "\n",
    "Es una descripción o imagen relacionados con un hecho, evento, personas, objetos u otras entidades del mundo real. El significado del dato cambia dependiendo dentro del contexto en que se encuentre. El siguiente ejercicio es el que usualmente utilizo en mis clases para la noción del concepto de datos.\n",
    "\n",
    "-   Considere el número **25...**\n",
    "\n",
    "-   Ahora... **25 \"Kilos\"**\n",
    "\n",
    "-   Y ahora... **25 \"kilos\" de \"papas\"**\n",
    "\n",
    "-   Finalmente... **25 \"kilos\" de \"papas\" en \"mercado\" de \"Concepción\"**\n",
    "\n",
    "### Contexto\n",
    "\n",
    "La búsqueda de datos para la generación de información se da dentro de cierto ámbito dentro de toda organización. Las organizaciones de cualquier índole utilizan e intercambian información, con sus usuarios/clientes y proveedores.\n",
    "\n",
    "![Posición de base de datos dentro de la organización.](images/informacion.jpg) \n",
    "\n",
    "En este nuevo contexto de alta dependencia de datos (eficientes y de calidad) se crean nuevos perfiles profesionales que suman a los ya tradicionales existentes en décadas pasadas en el ámbito del uso de las tecnologías de información.\n",
    "\n",
    "### Orígenes de datos\n",
    "\n",
    "La importancia de los datos es clara en su objetivo de otorgar información para la toma de decisiones, las que pueden llevar a una organización al éxito o fracaso de su gestión. Ahora bien, si es tan importante los datos también lo debeira ser el orígen de donde estos son extraídos, origenes que también han evolucionado en el tiempo. Hace no mas de 10 o más años la extracción de datos proviene de bases de datos expresamente diseñadas para ello, de planillas de cálculo (Excel preferentemente) llenadas a mano por usuarios destinados a ello y por formularios tanto papel como electrónicos, que finalmente terminaban en las bases y planillas antes mencionadas.\n",
    "\n",
    "Actualmente, se suman a las fuentes antes mencionadas un amplio número\n",
    "\n",
    "**Bases de datos**\n",
    "\n",
    "Una base de datos es un conjunto organizado de datos que se almacenan y gestionan en un sistema informático, permitiendo que la información se pueda buscar, consultar y actualizar fácilmente cuando se necesite.\n",
    "\n",
    "```{sql}\n",
    "-- Seleccionar toda la información de clientes de Chile.\n",
    "SELECT * FROM clientes WHERE pais = 'Chile'\n",
    "```\n",
    "\n",
    "**Archivos csv**\n",
    "\n",
    "El formato más asequible para acceder a datos es a través del intercambio de archivos de texto y separados por coma. Este es formato aceptado globalmente y es el tipo de intercambio de datos más utilizado entre dispositivos.\n",
    "\n",
    "\n",
    "### Perfiles y roles profesionales\n",
    "\n",
    "La complejidad de la obtención de datos y por tanto la complejidad de la información resultante ha llevado al desarrollo de un mayor número de perfiles especialistas, que hasta hace unos 10 años no se veían en el mercado profesional. El aumento de las fuentes de datos de todo tipo y forma, los nuevas formas de almacenamiento y de procesamiento han establecido una necesidad de mayores especialistas.\n",
    "\n",
    "#### Director de Datos (CDO)\n",
    "\n",
    "El Director de datos (**Chief Data Officer**) [@cdo] es un nuevo rol dentro de aquellas organizaciones[^01-intro-1] con una alta especialización y valoración de los datos. Es un puente entre el área comercial estratégica y el área TI que combina capacidades tecnológicas, estadísticas y gerenciales entre otras:\n",
    "\n",
    "[^01-intro-1]: <https://www2.deloitte.com/uy/es/pages/deloitte-analytics/articles/the-chief-data-officer.html>\n",
    "\n",
    "-   Entiende los datos y las necesidades de la empresa respecto a los datos.\n",
    "-   Decide qué datos deben almacenarse en la base de datos.\n",
    "-   Establece políticas para mantener y gestionar los datos almacenados.\n",
    "-   Gestiona los datos como valor estratégico de la organización.\n",
    "-   Establece las bases para el aseguramiento de la calidad de los datos.\n",
    "\n",
    "El conocimiento a cabalidad del área de negocios de la organización es fundamental para este perfil, ya que es quién guía a traves de todo el proceso de generación de información. Define los objetivos para la generación de valor del negocio respecto de la información y hace parte la analítica dentro del objetivo de negocio.\n",
    "\n",
    "> El rol fundamental del CDO se enfoca en sustentar la \"visión del negocio\" con información.\n",
    "\n",
    "También dentro de su área de acción se encuentra la gobernanza de los datos y el establecimiento de las políticas de uso de la información. Pasan de un rol de administrador a uno más estratégico e innovador que permite responder a los cambios tecnológicos cambiando sus ambiente de datos en la nuevas áreas de big data, automatización y aprendizaje de máquinas.\n",
    "\n",
    "El CDO dentro de la gobernanza de datos asesora en la implementación de políticas y coordina tanto lo requisitos como el control de la información sobre los restantes actores.\n",
    "\n",
    "![Áreas de un CDO.](images/cdo.png){fig-align=\"center\"}\n",
    "\n",
    "La creación de un CDO generalmente va acompañada de nuevos perfile asociados dentro de una estrategia de gobierno de datos. Estos nuevos perfiles son por lo general, el dueño de los datos y el custodio de datos.\n",
    "\n",
    "#### **Dueño de los Datos (Data owner)**\n",
    "\n",
    "El dueño de los datos dentro de la organización es un profesional interno de la organización, generalmente conocedor a un alto nivel del negocio de esta, de forma de poder establecer los datos correctos a ser utilizados y las mejoras necesarias para el crecimiento de la organización.\n",
    "\n",
    "Sus funciones no excluyentes son regularmente:\n",
    "\n",
    "-   Precisión y exactitud de información propia\n",
    "-   Definir reglas de uso y coordinación de todos los datos\n",
    "-   Identifica errores\n",
    "-   Define los niveles de calidad y de seguridad de la información\n",
    "-   Responsable de la confidencialidad y privacidad de la información\n",
    "-   Establece el valor de los datos para la organización\n",
    "\n",
    "#### **Custodio de Datos (Data Steward)**\n",
    "\n",
    "Con un aspecto más operativo que estratégico el custodio de los datos es aquél perfil que tiene entre sus responsabilidades la consistencia y accesibilidad a los datos que la organización requiere. Aparece en todas las definiciones aceptadas para este perfil (que pueden encontrarse en Internet), como un intermediario entre el Data Owner y el usuario final de los datos. Y que en organizaciones pequeñas puede estar asociadas al mismo perfil de un Data Owner.\n",
    "\n",
    "En su nivel operativo se encarga de:\n",
    "\n",
    "-   Captura, almacena y retiene información\n",
    "-   Calidad y disponibilidad\n",
    "-   Seguridad\n",
    "\n",
    "#### Data owner vs data steward\n",
    "\n",
    "#### **Arquitecto de Datos (Data Architect - DA)**\n",
    "\n",
    "Es tal la complejidad actual de los datos respecto de sus orígenes así como el volumen de datos que requieren ser transados, que se hace necesaria la participación de un nuevo perfil, el arquitecto de datos. Por sobre los perfiles mencionados anteriormente, es el DA el encargado de diseñar y orquestar las plataformas necesarias para el procesamiento masivo de datos para su transformación en información. Es un perfil netamente operativo encargado de construir y mantener la infraestructura de los datos de la organización.\n",
    "\n",
    "En general el DA es el encargado de diseñar y planificar el sistema de infraeastructura de datos y es el ingeniero de datos quien la construye, sin embargo en organizaciones pequeñas estos roles son realizados por el mismo cargo. Por lo que las funciones de un ingeniero de datos, que se detalla más adelante, se pueden consignar a un DA.\n",
    "\n",
    "Sus funciones principales abarcan:\n",
    "\n",
    "-   Diseño de modelos de datos según las reglas de negocio\n",
    "-   Desarrollo de bases de datos estableciendo el modelo conceptual, lógico y físico\n",
    "-   Determinar qué tecnologías va a usar y cómo va a hacerlo\n",
    "-   Crear procedimientos para garantizar la exactitud y la accesibilidad de los datos\n",
    "-   Seleccionar los almacenes de datos y las fuentes de los mismos\n",
    "-   Gestión del flujo de trabajo o workflow. Asegurarse de tener la velocidad de procesamiento y el acceso al almacenamiento para respaldarlo es esencial\n",
    "-   Encargado y principal responsable de que las tres fases de los procesos ETL se cumplan en los tiempos establecidos y se realicen correctamente\n",
    "-   Auditorías de datos: Realización de informes y evaluación del trabajo de forma habitual y asegurar la integridad de los datos.\n",
    "\n",
    "#### Ingeniero de Datos (Data Engineer - DE)\n",
    "\n",
    "El trabajo del ingeniero de datos es \"la representación y el movimiento de datos para que sean consumibles y utilizables\", dijo Pope. El ingeniero de datos, debe poder tomar los datos sin procesar, limpiarlos, moverlos a una base de datos, etiquetarlos y, en general, asegurarse de que estén listos para la siguiente etapa del proceso.\n",
    "\n",
    "Con un amplio manejo de Big Data, este perfil es sumamente técnico. Los ingenieros de datos se encuentran entre los desarrolladores de aplicaciones y los científicos de datos. Se encargan de diseñar, construir y gestionar los datos y la infraestructura necesaria para almacenarlos y procesarlos. Construyen la base tecnológica para que los científicos de datos y analistas puedan realizar sus tareas. Por lo tanto, son los responsables de mantener sistemas escalables, con alta disponibilidad y rendimiento, integrando nuevas tecnologías y desarrollando el software necesario.\n",
    "\n",
    "Deben conocer las tecnologías Big Data Apache Spark, Scala, Docker, Hadoop, HDFS y otras, y entender cómo se integran sus tecnologías y las formas de procesar, transformar y tratar los datos con herramientas de ingesta y los procesos ETL. Además, deben saber cómo mover datos hacia y desde los sistemas de Big data y la implementación de bases de datos para grandes volumenes de datos. Entre sus funciones también se encuentra dar apoyo y facilitar el trabajo a analistas y científicos de datos, así como al negocio.\n",
    "\n",
    "ETL. Procesos de extracción, transformación y carga de datos.\n",
    "\n",
    "Como se trata de procesos complejos y muy integrados se requiere además un manejo de lenguajes de scripting y conocer procesos de automatización (mediante Python u otro lenguaje) e interacciones con APIs y fuentes de datos externas.\n",
    "\n",
    "Por definición, el Big Data suele tener lugar en sistemas distribuidos, que es otro de los conocimientos fundamentales para un buen ingeniero de datos. Estos sistemas tienen numerosas particularidades en torno a la replicación de datos, consistencia, tolerancia a fallos, particionado y concurrencia. En este punto se englobarían tecnologías como HDFS, Hadoop o Spark.\n",
    "\n",
    "Habilidades Fundamentales Tecnologías y servicios Cloud. La demanda de estas tecnologías no para de crecer, y es que cada vez es más frecuente iniciar proyectos de migración a la nube en las empresas. Un buen ingeniero de datos debe conocer y tener experiencia en el uso de servicios cloud, sus ventajas, desventajas y sus aplicación en proyectos Big Data. Al menos debería estar familiarizado con una plataforma como Azure o AWS ya que son las más extendidas. Además, debe conocer buenas prácticas en cuanto a seguridad de los datos y virtualización. Recuerda que estas tecnologías han venido para quedarse e invertir tiempo en formarse es siempre una buena idea.\n",
    "\n",
    "Los Ingenieros de Datos también deben conocer el funcionamiento y uso de las bases de datos. También las diferencias que existen entre bases de datos relacionales y NoSQL. El lenguaje básico para interactuar con estas bases de datos es SQL, por lo que también debe estar familiarizado con escribir consultas de lectura y manipulación de datos. Además, debe entender la diferencia entre los tipos de bases de datos NoSQL y los casos de uso para cada uno de ellos.\n",
    "\n",
    "Uno de los roles principales de los ingenieros de datos es crear pipelines de datos con tecnologías ETL y frameworks de orquestación. En esta sección podríamos enumerar muchas tecnologías pero el ingeniero de datos debería conocer o sentirse cómodo con algunas de las más conocidas como puede ser [NiFi](https://nifi.apache.org/) o [Airflow](https://airflow.apache.org/).\n",
    "\n",
    "Las actividades esenciales para este perfil profesional son entre otras y mezcladas habitualmente con las del DA:\n",
    "\n",
    "-   Diseño de modelos de datos\n",
    "-   Desarrollo de bases de datos\n",
    "-   Tecnologías Big Data\n",
    "-   Seleccionar los almacenes de datos y las fuentes de los mismos\n",
    "-   Gestión del flujo de trabajo (workflow). Asegurarse de tener la velocidad de procesamiento y el acceso al almacenamiento para respaldarlo es esencial\n",
    "-   Responsabilidad sobre las tres fases de los [procesos ETL](https://www.inesdi.com/blog/etl-que-es-procesos-y-herramientas/) se cumplan en los tiempos establecidos y se realicen correctamente\n",
    "-   Auditorías de datos: realización de informes y evaluación del trabajo de forma habitual\n",
    "-   Manejo de herramientas de gestión: Apache Spark, Hadoop, Airflow, etc.\n",
    "\n",
    "#### Analista de Datos (Data Analyst)\n",
    "\n",
    "Este perfil profesional esta abocado a la recopilación y transformación de datos para la toma de decisiones empresariales. Para ello se vale del uso de estadística y de herramientas de gestión de datos para presentar hechos y respuestas a preguntas del negocio. Las habilidades de comunicación son relevantes para este perfil junto al conocimiento del negocio, ya que debe informar y explicar la información obtenida para la toma de decisiones.\n",
    "\n",
    "Responde a preguntas tales como ¿cuántas ventas se realizaron este mes? ¿cuáles han sido los productos más solicitados y donde?, ¿cuál es la proyección de ingresos para 2024?, ¿qué volumen de contactos se ha transformado en ventas?, ¿qué modelo de ventas ha sido más exitoso?.\n",
    "\n",
    "Este perfil profesional posee una alta capacidad para la gestión de captura de datos y la generarción de procesos de transformación de datos, con conocimientos de uso de bases de datos, captura de datos mediante SQL y Excel, y el uso de herramientas de programación como Python y R entre otras.\n",
    "\n",
    "La herramienta más poderosa de un analista es un gestor de reportes, del tipo [Power BI](https://powerbi.microsoft.com/es-es/desktop/) o [Tableau](https://www.tableau.com/es-mx) (entre otros), que permite realizar todo tipo de informes o completos paneles de información, orientados a comunicar de forma efectiva los resultados con el fin de la toma de decisiones.\n",
    "\n",
    "Las actividades fundamentales o capacidades de este tipo de perfil son:\n",
    "\n",
    "-   Preprocesamiento de datos\n",
    "-   Análisis exploratorio\n",
    "-   Estadística descriptiva e inferencial\n",
    "-   Visualización de datos\n",
    "-   Comunicación de resultados, siendo esta tal vez la función más relevante para los fines de la organización\n",
    "\n",
    "#### Científico de Datos (Data Scientist)\n",
    "\n",
    "A diferencia de un analista de datos un científico de datos está orientado a resolver problemas del tipo de descubrimiento de patrones. Para lo cual se vale de herramientas de programación, estadística probabilística y técnicas de aprendizaje automático.\n",
    "\n",
    "No es el volumen de datos lo que distingue a un analista de un científico (de datos) sino el objetivo a cumplir con esos datos. El analista presenta el estado actual y posiblemente la proyección simple (inferencia) de lo que puede ocurrir en un futuro cercano. En tanto, el científico de datos debe realizar descubrimiento de patrones, plantear hipótesis y verificarlas (uso del método científico).\n",
    "\n",
    "#### Administrador de Base de datos (DBA)\n",
    "\n",
    "El DBA (Data Base Administrator) es el profesional informático encargado de la administración de una o varias bases de datos gestionando su uso y funcionamiento. Es responsable por el diseño de la base de datos y la gestión de ella, fijando normas que resguardan tanto la seguridad como la integridad de ellas.\n",
    "\n",
    "Este perfil es netamente operativo a cargo de la infraestructura directa asociada a las bases de datos de la organización. Siendo el responsable de la mantención de la estructura y las políticas de seguridad de las bases. Es este perfil el gestiona además los roles de acceso y permisos asociados a los usuarios de la organización definidos por el DA y/o el DO.\n",
    "\n",
    "Este perfil es, en última instancia el encargado de la creación, edición de las bases de datos y su estructura. También de la construcción de los procedimientos y funciones requeridos para su funcionamiento.\n",
    "\n",
    "**Funciones**\n",
    "\n",
    "-   Crea la base de datos.\n",
    "-   Implementa los controles necesarios para que se respeten las políticas establecidas por el administrador de datos.\n",
    "-   Es el responsable de garantizar que el sistema obtenga las prestaciones deseadas. Presta servicios técnicos.\n",
    "-   Mantener la base de datos disponible y actualizada.\n",
    "-   Realizar los respaldos de seguridad. Define políticas de seguridad y de respaldo.\n",
    "-   Disponer del accesos a los datos desde las aplicaciones.\n",
    "-   Mantener la seguridad de los datos.\n",
    "-   Diseñar y administrar la estructura de los datos.\n",
    "-   Monitorear la actividad de los datos.\n",
    "-   Se asegura de que la comunicación del sistema con la base de datos sea expedita.\n",
    "\n",
    "Los Administradores de Bases de Datos son responsables del manejo, mantenimiento, desempeño y de la confiabilidad de bases de datos. Asimismo, están a cargo de la mejora y diseño de nuevos modelos de las mismas. Manejar una base de datos implica recolectar, clasificar y resguardar la información de manera organizada, por ello, estos profesionales velan por garantizar que la misma esté debidamente almacenada y segura, además de que sea de fácil acceso cuando sea necesario.\n",
    "\n",
    "#### Desarrollador de Base de Datos\n",
    "\n",
    "Personas como analistas de sistemas y programadores que diseñan nuevos programas de aplicación para los usuarios finales.\n",
    "\n",
    "Los programadores de sistemas informáticos escriben programas para controlar el funcionamiento interno de los ordenadores, lo que implica diseñar programas que sean eficientes, rápidos y versátiles. Dedican mucho tiempo a probar los programas, y también puede instalar, personalizar y dar soporte a estos sistemas operativos.\n",
    "\n",
    "El profesional que debiera asumir este rol es:\n",
    "\n",
    "-   Ingeniero en informática\n",
    "-   Programador\n",
    "-   Analista programador\n",
    "\n",
    "#### Usuario final\n",
    "\n",
    "Los usuarios finales son las personas que utilizan los datos para su trabajo cotidiano y no son necesariamente del área de la informática. Normalmente no utilizan la base de datos directamente, sino aplicaciones creadas para ellos a fin de facilitar la manipulación de los datos. Estos usuarios sólo acceden a ciertos datos del total.\n",
    "\n",
    "Si bien el usuario final es el receptor de lso datos/información preparada por los perfiles antes mencionados, también en su actividad diaria pueden generar necesidades para la toma de nuevos y/o mejorados datos para cumplimentar sus funciones.\n",
    "\n",
    "### La información\n",
    "\n",
    "Información son datos que han sido organizados o preparados en una forma adecuada para apoyar la toma de decisiones: Por ejemplo una lista de productos y su stock sin ningún orden son datos, pero un lista de productos ordenados por stock (de menor a mayor) representa información para el encargado de compras de un supermercado.\n",
    "\n",
    "![Los datos por si solos no conducen a información.](images/conocimiento.jpg)\n",
    "\n",
    "### Discusión\n",
    "\n",
    "¿Puede en un momento dado un objeto considerarse como dato y en otro momento como información? ¿Por qué?\n",
    "\n",
    "![Datos e información.](images/datoinfo.png) \n",
    "\n",
    "**Tarea**\n",
    "\n",
    "¿Es capaz de presentarse a sí mismo sin entregar información si no solo datos?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f61ab",
   "metadata": {},
   "source": [
    "## Historia y definición de Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ec60c",
   "metadata": {},
   "source": [
    "## Conceptos fundamentales y tipos de análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baec0cb",
   "metadata": {},
   "source": [
    "## Flujo de trabajo de un proyecto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88371b",
   "metadata": {},
   "source": [
    "\n",
    "## Ciclo de Vida de un Proyecto de Data Science\n",
    "\n",
    "El ciclo de vida de un proyecto de Data Science describe las etapas clave desde la concepción del problema hasta la entrega de soluciones accionables basadas en datos.\n",
    "\n",
    "### 🧭 Etapas Generales\n",
    "\n",
    "1. **Definición del Problema**\n",
    "2. **Recolección de Datos**\n",
    "3. **Preparación y Limpieza de Datos**\n",
    "4. **Análisis Exploratorio (EDA)**\n",
    "5. **Modelado**\n",
    "6. **Evaluación del Modelo**\n",
    "7. **Despliegue**\n",
    "8. **Comunicación de Resultados**\n",
    "9. **Mantenimiento**\n",
    "\n",
    "\n",
    "### CRISP-DM\n",
    "**CRISP-DM** (Cross-Industry Standard Process for Data Mining) es una metodología desarrollada a finales de los años 90 por un consorcio liderado por IBM, NCR y Daimler-Benz, con el objetivo de estandarizar los procesos de minería de datos en la industria. Su origen se remonta a la necesidad de crear un marco común y estructurado que facilitara la aplicación de técnicas de análisis de datos en distintos sectores, permitiendo replicabilidad y mejores prácticas.\n",
    "\n",
    "El desarrollo de CRISP-DM fue financiado por la Comisión Europea y publicado oficialmente en 1999. Desde entonces, se ha convertido en el estándar de facto para proyectos de ciencia de datos y minería de datos, gracias a su enfoque flexible y adaptable.\n",
    "\n",
    "La metodología CRISP-DM se compone de seis fases principales:\n",
    "\n",
    "1. **Comprensión del negocio (Business Understanding):** Definir los objetivos del proyecto desde la perspectiva del negocio y traducirlos en una problemática de análisis de datos.\n",
    "2. **Comprensión de los datos (Data Understanding):** Recolectar, describir y explorar los datos iniciales para identificar problemas de calidad y obtener insights preliminares.\n",
    "3. **Preparación de los datos (Data Preparation):** Seleccionar, limpiar, transformar y construir los datos necesarios para el modelado.\n",
    "4. **Modelado (Modeling):** Seleccionar y aplicar técnicas de modelado, calibrando los parámetros para obtener los mejores resultados.\n",
    "5. **Evaluación (Evaluation):** Revisar los modelos generados para asegurar que cumplen los objetivos del negocio y decidir los próximos pasos.\n",
    "6. **Despliegue (Deployment):** Implementar el modelo en el entorno productivo y asegurar su integración con los procesos de negocio.\n",
    "\n",
    "CRISP-DM destaca por su carácter iterativo: las fases no son estrictamente secuenciales y es común regresar a etapas anteriores según los hallazgos y necesidades del proyecto. Su éxito radica en la claridad de sus etapas y en su aplicabilidad a proyectos de cualquier industria, promoviendo la comunicación efectiva entre equipos técnicos y de negocio.\n",
    "\n",
    "### OSEMN\n",
    "\n",
    "OSEMN es un acrónimo propuesto por Hilary Mason y Chris Wiggins en 2010 para describir de manera práctica y sencilla el flujo de trabajo típico en proyectos de ciencia de datos. El término aparece por primera vez en el artículo “A Taxonomy of Data Science” publicado en *The Data Science Handbook* y popularizado en el blog de O’Reilly.\n",
    "\n",
    "OSEMN representa cinco etapas fundamentales:\n",
    "\n",
    "1. **Obtain (Obtener):** Recolectar los datos necesarios desde diversas fuentes, como bases de datos, APIs, archivos planos, web scraping, etc.\n",
    "2. **Scrub (Limpiar):** Procesar y limpiar los datos para corregir errores, eliminar duplicados, tratar valores faltantes y asegurar la calidad de la información.\n",
    "3. **Explore (Explorar):** Analizar los datos de manera exploratoria mediante estadísticas descriptivas y visualizaciones para identificar patrones, tendencias y anomalías.\n",
    "4. **Model (Modelar):** Aplicar técnicas de modelado estadístico o de machine learning para extraer conocimiento, hacer predicciones o clasificaciones.\n",
    "5. **iNterpret (Interpretar):** Traducir los resultados del modelo en conclusiones accionables, comunicar hallazgos y generar valor para la toma de decisiones.\n",
    "\n",
    "**Orígenes y usos:**  \n",
    "OSEMN surge como una respuesta a la necesidad de un marco ágil y comprensible para quienes inician en la ciencia de datos, especialmente en entornos educativos, startups y bootcamps. Su enfoque es eminentemente práctico y técnico, priorizando la manipulación y análisis de datos sobre la gestión de proyectos o la alineación con objetivos de negocio. Es ampliamente utilizado para enseñar el flujo de trabajo esencial de un científico de datos y como guía rápida en proyectos de prototipado o análisis exploratorio.\n",
    "\n",
    "\n",
    "### 🔁 Comparativa entre CRISP-DM y OSEMN\n",
    "\n",
    "| Aspecto                    | CRISP-DM                                              | OSEMN                                                   |\n",
    "|----------------------------|--------------------------------------------------------|----------------------------------------------------------|\n",
    "| Acrónimo                   | Cross-Industry Standard Process for Data Mining        | Obtain, Scrub, Explore, Model, iNterpret                |\n",
    "| Enfoque principal          | Minería de datos, empresarial                         | Práctico y técnico                                       |\n",
    "| Etapas                     | 6: Business Understanding, Data Understanding, etc.    | 5: Obtain, Scrub, Explore, Model, Interpret              |\n",
    "| Inclusión de negocio       | Explícita                                              | Implícita                                                |\n",
    "| Nivel de formalización     | Alto                                                   | Medio-bajo                                               |\n",
    "\n",
    "### Team Data Science Process (TDSP)\n",
    "\n",
    "**TDSP** es una metodología desarrollada por Microsoft para estructurar, gestionar y escalar proyectos de ciencia de datos en equipos colaborativos. TDSP proporciona un marco de trabajo integral que abarca desde la definición del problema hasta el despliegue y mantenimiento de soluciones analíticas, integrando buenas prácticas de ingeniería de software, DevOps y gestión de proyectos.\n",
    "\n",
    "**Orígenes**\n",
    "\n",
    "TDSP surge en 2016 como respuesta a la necesidad de Microsoft de estandarizar y optimizar el ciclo de vida de los proyectos de ciencia de datos en entornos empresariales, especialmente en la nube de Azure. Fue diseñado para facilitar la colaboración entre científicos de datos, ingenieros de datos y desarrolladores, promoviendo la reutilización de código, la trazabilidad y la integración continua.\n",
    "\n",
    "**Fases principales de TDSP**\n",
    "\n",
    "1. **Business Understanding** (Comprensión del negocio): Definir objetivos, métricas de éxito y criterios de aceptación alineados con las necesidades del negocio.\n",
    "2. **Data Acquisition and Understanding** (Adquisición y comprensión de datos): Recolectar, explorar y validar los datos relevantes para el problema.\n",
    "3. **Modeling** (Modelado): Desarrollar, entrenar y validar modelos predictivos o de análisis.\n",
    "4. **Deployment** (Despliegue): Implementar los modelos en entornos productivos, asegurando su integración con aplicaciones y procesos existentes.\n",
    "5. **Customer Acceptance** (Aceptación del cliente): Validar los resultados con los usuarios finales y ajustar según retroalimentación.\n",
    "\n",
    "**Usos y ventajas**\n",
    "\n",
    "- **Colaboración:** Facilita el trabajo en equipo mediante repositorios estructurados, control de versiones y documentación estandarizada.\n",
    "- **Escalabilidad:** Permite gestionar proyectos complejos y repetibles, integrando herramientas de Azure y DevOps.\n",
    "- **Trazabilidad:** Documenta cada etapa del proyecto, facilitando auditorías y mejoras continuas.\n",
    "- **Despliegue ágil:** Incluye prácticas para el despliegue automatizado y mantenimiento de modelos en producción.\n",
    "\n",
    "TDSP es ampliamente utilizado en organizaciones que trabajan con Azure y buscan profesionalizar la gestión de proyectos de ciencia de datos, especialmente en contextos donde la colaboración, la gobernanza y la integración con sistemas empresariales son prioritarias.\n",
    "\n",
    "\n",
    "### AWS Data Science Lifecycle\n",
    "\n",
    "El **AWS Data Science Lifecycle** es el marco metodológico propuesto por Amazon Web Services para gestionar proyectos de ciencia de datos en la nube. Este ciclo de vida está diseñado para aprovechar la infraestructura escalable, los servicios gestionados y las capacidades de automatización que ofrece AWS, facilitando la implementación de soluciones de machine learning y análisis avanzado de datos.\n",
    "\n",
    "**Fases principales del AWS Data Science Lifecycle:**\n",
    "\n",
    "1. **Problem Definition (Definición del problema):** Identificación clara de los objetivos de negocio y los resultados esperados, alineando el proyecto con las necesidades de la organización.\n",
    "2. **Data Collection (Recolección de datos):** Obtención de datos desde diversas fuentes, como bases de datos, almacenamiento en la nube, APIs o flujos en tiempo real, utilizando servicios como AWS S3, AWS Glue o Amazon RDS.\n",
    "3. **Data Preparation (Preparación de datos):** Limpieza, transformación y enriquecimiento de los datos mediante herramientas como AWS Glue, Amazon SageMaker Data Wrangler o scripts personalizados.\n",
    "4. **Model Building (Construcción del modelo):** Desarrollo, entrenamiento y ajuste de modelos de machine learning utilizando Amazon SageMaker, que permite gestionar entornos, recursos y experimentos de manera eficiente.\n",
    "5. **Model Deployment (Despliegue del modelo):** Implementación de modelos en producción a través de endpoints gestionados, integración con aplicaciones o automatización de inferencias usando servicios como SageMaker Endpoint o AWS Lambda.\n",
    "6. **Monitoring & Maintenance (Monitoreo y mantenimiento):** Supervisión continua del rendimiento del modelo, detección de desviaciones (drift), actualización y retraining automático, aprovechando herramientas como Amazon CloudWatch y SageMaker Model Monitor.\n",
    "\n",
    "**Usos principales:**\n",
    "\n",
    "- **Desarrollo y despliegue de modelos de machine learning** en entornos productivos y escalables.\n",
    "- **Automatización de flujos de trabajo de ciencia de datos** mediante pipelines gestionados.\n",
    "- **Integración con servicios cloud** para análisis en tiempo real, procesamiento masivo y almacenamiento seguro.\n",
    "- **Colaboración entre equipos** mediante entornos compartidos y control de versiones.\n",
    "\n",
    "**Ventajas:**\n",
    "\n",
    "- **Escalabilidad:** Permite manejar grandes volúmenes de datos y cargas de trabajo variables sin preocuparse por la infraestructura.\n",
    "- **Automatización:** Facilita la creación de pipelines de datos y modelos, reduciendo errores manuales y acelerando el ciclo de vida.\n",
    "- **Integración nativa:** Conecta fácilmente con otros servicios de AWS, optimizando el flujo de trabajo de extremo a extremo.\n",
    "- **Despliegue ágil:** Proporciona herramientas para el despliegue rápido y seguro de modelos en producción.\n",
    "- **Monitoreo avanzado:** Ofrece capacidades integradas para supervisar y mantener modelos en producción.\n",
    "\n",
    "**Desventajas respecto a otros ciclos:**\n",
    "\n",
    "- **Dependencia de la nube AWS:** El ciclo está fuertemente ligado al ecosistema de Amazon, lo que puede limitar la portabilidad a otras plataformas.\n",
    "- **Curva de aprendizaje:** Requiere familiaridad con los servicios y herramientas específicas de AWS.\n",
    "- **Costos:** El uso intensivo de servicios gestionados puede incrementar los costos operativos si no se optimizan adecuadamente.\n",
    "- **Menor foco en la gestión de negocio:** A diferencia de CRISP-DM o TDSP, el ciclo de AWS enfatiza la implementación técnica y automatización, dejando en segundo plano la alineación estratégica con el negocio.\n",
    "\n",
    "En resumen, el AWS Data Science Lifecycle es ideal para organizaciones que buscan aprovechar la nube para escalar, automatizar y agilizar proyectos de ciencia de datos, especialmente cuando la integración y el despliegue continuo son prioritarios. Sin embargo, puede no ser la mejor opción para equipos que requieren independencia de proveedor o una gestión de proyectos más orientada al negocio.\n",
    "\n",
    "\n",
    "## 📊 Comparativa Extendida: CRISP-DM, OSEMN, TDSP, AWS\n",
    "\n",
    "| Característica                       | CRISP-DM                    | OSEMN                       | TDSP (Microsoft)              | AWS Data Science Lifecycle       |\n",
    "|-------------------------------------|-----------------------------|-----------------------------|-------------------------------|----------------------------------|\n",
    "| Origen                              | IBM                         | Mason & Wiggins (~2010)     | Microsoft Azure               | Amazon Web Services              |\n",
    "| Foco                                | Negocio y minería de datos | Práctico-técnico            | Gestión colaborativa          | Escalabilidad en la nube         |\n",
    "| Etapas                              | 6                           | 5                           | 5                              | 6                                |\n",
    "| Gestión de proyectos                | Media                       | Baja                        | Alta                           | Alta                             |\n",
    "| Colaboración equipo                 | Limitada                    | Individual                  | Fuerte                         | Fuerte                           |\n",
    "| Despliegue/Mantenimiento            | Débil                       | No considerado              | Formal con DevOps             | Automatizado con AWS             |\n",
    "| Uso en industria                    | Consultorías, banca         | Startups, bootcamps         | Organizaciones Azure           | Organizaciones con AWS           |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Recomendación de Uso\n",
    "\n",
    "| Escenario                                     | Metodología sugerida |\n",
    "|----------------------------------------------|----------------------|\n",
    "| Proyectos estructurados                      | CRISP-DM             |\n",
    "| Startups o prototipos rápidos                | OSEMN                |\n",
    "| Equipos corporativos con Azure               | TDSP                 |\n",
    "| Proyectos cloud con MLOps                    | AWS Lifecycle        |\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Referencias\n",
    "\n",
    "- CRISP-DM: https://www.the-modeling-agency.com/crisp-dm.pdf\n",
    "- OSEMN: *Doing Data Science* (O'Reilly)\n",
    "- TDSP: https://docs.microsoft.com/en-us/azure/architecture/data-science-process/\n",
    "- AWS: https://docs.aws.amazon.com/sagemaker/latest/dg/data-science-process.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d38e2",
   "metadata": {},
   "source": [
    "## Ética y privacidad en datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d7fe5c",
   "metadata": {},
   "source": [
    "## Herramientas y entornos comunes (Python, Jupyter, Git, VS Code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
