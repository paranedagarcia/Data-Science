{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92dedcd3-c3cd-4c4d-a609-deb41b360ed2",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "El **Gradient Boosting** es una técnica de ensamblado (*ensemble*) utilizada en machine learning para construir modelos predictivos potentes a partir de la combinación secuencial de modelos débiles, generalmente árboles de decisión poco profundos. El objetivo es mejorar la precisión corrigiendo los errores cometidos por los modelos anteriores en cada iteración.\n",
    "\n",
    "**Definición matemática:**  \n",
    "Gradient Boosting busca minimizar una función de pérdida $L(y, F(x))$ (por ejemplo, el error cuadrático para regresión o la log-loss para clasificación) mediante la construcción de un modelo aditivo:\n",
    "\n",
    "$$\n",
    "F_{m}(x) = F_{m-1}(x) + \\gamma_m h_m(x)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $F_{m}(x)$ es el modelo en la iteración $m$,\n",
    "- $F_{m-1}(x)$ es el modelo anterior,\n",
    "- $h_m(x)$ es el nuevo modelo débil (por ejemplo, un árbol de decisión),\n",
    "- $\\gamma_m$ es el peso (tasa de aprendizaje) asignado a $h_m(x)$.\n",
    "\n",
    "En cada iteración, $h_m(x)$ se ajusta para aproximar el gradiente negativo de la función de pérdida respecto a las predicciones actuales, es decir, el error residual:\n",
    "\n",
    "$$\n",
    "r_{im} = -\\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)}\n",
    "$$\n",
    "\n",
    "El nuevo modelo $h_m(x)$ se entrena para predecir estos residuos.\n",
    "\n",
    "**Ejemplo de uso en Python (regresión):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Entrenamiento del modelo Gradient Boosting\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=2)\n",
    "gb.fit(X, y)\n",
    "\n",
    "# Predicción\n",
    "y_pred = gb.predict(X)\n",
    "print(\"Predicciones:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b42a40e",
   "metadata": {},
   "source": [
    "**Aplicaciones comunes:**  \n",
    "Gradient Boosting se utiliza ampliamente en tareas de regresión y clasificación, como predicción de precios, scoring crediticio, detección de fraudes y competiciones de ciencia de datos (por ejemplo, XGBoost, LightGBM y CatBoost son variantes populares). Su fortaleza radica en su alta precisión y capacidad para manejar datos heterogéneos y relaciones complejas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
