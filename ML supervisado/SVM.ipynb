{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffbdd5bb",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "Una Máquina de Vectores de Soporte (SVM) es un tipo de algoritmo de inteligencia artificial y aprendizaje automático (machine learning) que se usa para clasificar datos o predecir valores. El objetivo de SVM es encontrar el hiperplano óptimo que separa las clases de datos con el mayor margen posible. Este margen es la distancia entre el hiperplano y los puntos más cercanos de cada clase, llamados *vectores de soporte*. En otras palabras, imagínalo como un \"delineador inteligente\" que dibuja la mejor línea (o superficie) para separar grupos de cosas diferentes, como distinguir entre correos spam y no spam, o entre imágenes de perros y gatos.\n",
    "\n",
    "**Orígenes**\n",
    "\n",
    "Las SVM fueron propuestas por primera vez en los años 90 por los científicos [Vladimir Vapnik](https://en.wikipedia.org/wiki/Vladimir_Vapnik) y [Alexey Chervonenkis](https://en.wikipedia.org/wiki/Alexey_Chervonenkis).\n",
    "\n",
    "Surgieron como una mejora a otros métodos de clasificación, buscando una solución más robusta y eficiente, especialmente cuando los datos son complejos.\n",
    "\n",
    "**¿Cómo funciona?**\n",
    "\n",
    "Separación con margen: La SVM busca la línea (o plano, en 3D) que mejor separa dos grupos, pero no cualquier línea: la que deja el mayor espacio posible (\"margen\") entre ambos grupos.\n",
    "\n",
    "Ejemplo: Si tienes manzanas (rojas) y naranjas (naranjas) en una mesa, la SVM dibuja una línea que las separa, pero alejada lo más posible de ambas para evitar confusiones.\n",
    "\n",
    "Kernel Trick (Truco del núcleo): Si los datos no se pueden separar con una línea recta (como un ovillo de hilos mezclados), la SVM usa funciones matemáticas (\"kernels\") para transformarlos a un espacio donde sí sea posible separarlos.\n",
    "\n",
    "\n",
    "**Definición matemática:**\n",
    "\n",
    "Dado un conjunto de entrenamiento $\\{(x_i, y_i)\\}_{i=1}^N$, donde $x_i \\in \\mathbb{R}^d$ y $y_i \\in \\{-1, 1\\}$, SVM busca el hiperplano $w^T x + b = 0$ que maximiza el margen, resolviendo el siguiente problema de optimización:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "sujeto a:\n",
    "\n",
    "$$\n",
    "y_i (w^T x_i + b) \\geq 1 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "Para casos no lineales, SVM utiliza funciones *kernel* para proyectar los datos a un espacio de mayor dimensión donde sean separables linealmente.\n",
    "\n",
    "**Ejemplos de uso:**\n",
    "\n",
    "- **Clasificación de texto:** Detección de spam, análisis de sentimientos (positivo/negativo).\n",
    "- **Reconocimiento de imágenes:** Identificación de objetos o dígitos manuscritos.\n",
    "- **Bioinformática:** Clasificación de genes o proteínas, clasificar tipos de células.\n",
    "- **Detección de fraudes:** Identificación de transacciones anómalas, predecir riesgos de crédito o fraudes.\n",
    "\n",
    "SVM es especialmente útil en problemas con alta dimensionalidad y cuando el número de muestras es menor que el número de características.\n",
    "\n",
    "**Ventajas de SVM:**\n",
    "\n",
    "- Efectivo en espacios de alta dimensión y cuando el número de dimensiones es mayor que el número de muestras. Funciona bien incluso cuando la separación entre grupos no es obvia.\n",
    "- Utiliza solo un subconjunto de puntos de entrenamiento (vectores de soporte), lo que lo hace eficiente en memoria.\n",
    "- Flexible gracias al uso de diferentes funciones *kernel* para problemas lineales y no lineales. Gracias al kernel trick, puede manejar relaciones no lineales.\n",
    "- Generalmente robusto frente al sobreajuste, especialmente en problemas bien separados.\n",
    "\n",
    "**Desventajas de SVM:**\n",
    "\n",
    "- El entrenamiento puede ser lento y costoso en memoria para conjuntos de datos muy grandes.\n",
    "- La elección del *kernel* y sus parámetros puede ser compleja y requiere ajuste cuidadoso. Elegir el kernel y ajustar sus configuraciones requiere experiencia.\n",
    "- Menor interpretabilidad del modelo en comparación con modelos lineales simples o árboles de decisión. A diferencia de un árbol de decisiones, es menos claro cómo toma las decisiones.\n",
    "- No proporciona directamente probabilidades de clase (aunque se pueden estimar con métodos adicionales).\n",
    "\n",
    "**¿Qué son las funciones Kernel en SVM?**  \n",
    "\n",
    "Las **funciones kernel** permiten a las SVM resolver problemas no lineales proyectando los datos originales a un espacio de mayor dimensión donde sí pueden separarse linealmente. El kernel calcula el producto interno entre las imágenes de los datos en ese espacio, sin necesidad de calcular explícitamente la transformación (truco del kernel).\n",
    "\n",
    "**Principales tipos de kernel:**\n",
    "\n",
    "- **Lineal:** $K(x, x') = x^T x'$. No transforma los datos, útil si son linealmente separables.\n",
    "- **Polinómico:** $K(x, x') = (x^T x' + c)^d$. Permite separar datos con fronteras polinómicas.\n",
    "- **RBF (Radial Basis Function o Gaussiano):** $K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)$. Muy usado, permite separar datos con fronteras complejas y no lineales.\n",
    "- **Sigmoide:** $K(x, x') = \\tanh(\\alpha x^T x' + c)$. Similar a una neurona de red neuronal.\n",
    "\n",
    "La elección del kernel y sus parámetros es clave para el rendimiento de SVM. El kernel RBF es el más común en la práctica para problemas no lineales.\n",
    "\n",
    "**¿Qué parámetros se ajustan en el kernel RBF?**  \n",
    "\n",
    "Los principales parámetros del kernel RBF (Radial Basis Function) en SVM son:\n",
    "\n",
    "- **C (parámetro de regularización):** Controla el equilibrio entre maximizar el margen y minimizar el error de clasificación. Un valor bajo de C permite un margen más amplio pero más errores; un valor alto busca clasificar correctamente todos los ejemplos, pero puede sobreajustar.\n",
    "- **gamma ($\\gamma$):** Determina la influencia de cada muestra de entrenamiento. Un valor bajo de gamma implica que cada punto tiene un alcance amplio (frontera más suave), mientras que un valor alto hace que la influencia sea muy local (frontera más compleja y ajustada a los datos).\n",
    "\n",
    "El ajuste de estos parámetros es fundamental para el rendimiento del modelo y suele realizarse mediante validación cruzada y búsqueda en malla (*GridSearchCV*).\n",
    "\n",
    "**Ejemplo cotidiano**\n",
    "\n",
    "Imagina que tienes dos tipos de semillas en una mesa: redondas y alargadas. Una SVM:\n",
    "\n",
    "1. Mide características como tamaño y peso.\n",
    "2. Dibuja una línea que las separa lo mejor posible.\n",
    "3. Si llega una semilla nueva, la clasifica según en qué lado de la línea caiga.\n",
    "\n",
    "Las SVM son como \"superreglas\" que aprenden a dividir datos de la manera más precisa posible. Son poderosas en tareas de clasificación, pero su rendimiento depende de la calidad de los datos y la configuración elegida."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
