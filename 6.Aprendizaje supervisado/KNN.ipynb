{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "378c829a-7e0a-4f3c-b6d9-6d1b47d3f614",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)\n",
    "El **K-Nearest Neighbors** es un algoritmo de aprendizaje supervisado utilizado tanto para clasificación como para regresión. Su principio fundamental es que una muestra se clasifica o predice en función de las etiquetas o valores de sus *k* vecinos más cercanos en el espacio de características.\n",
    "\n",
    "**Definición matemática:**  \n",
    "Dado un conjunto de entrenamiento $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$, donde $x_i \\in \\mathbb{R}^d$ y $y_i$ es la etiqueta (clase o valor), para una nueva muestra $x$:\n",
    "\n",
    "1. Se calcula la distancia (usualmente Euclídea) entre $x$ y cada $x_i$ del conjunto de entrenamiento.\n",
    "2. Se seleccionan los $k$ puntos más cercanos a $x$.\n",
    "3. - **Clasificación:** $x$ se asigna a la clase más frecuente entre sus $k$ vecinos.\n",
    "    - **Regresión:** $x$ toma el valor promedio (o ponderado) de los valores de sus $k$ vecinos.\n",
    "\n",
    "Formalmente, para clasificación:\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{c} \\sum_{i \\in N_k(x)} \\mathbb{I}(y_i = c)\n",
    "$$\n",
    "donde $N_k(x)$ es el conjunto de índices de los $k$ vecinos más cercanos a $x$, y $\\mathbb{I}$ es la función indicadora.\n",
    "\n",
    "**Ejemplos de uso:**\n",
    "- **Reconocimiento de dígitos manuscritos:** Clasificar imágenes de dígitos según los vecinos más similares en el espacio de píxeles.\n",
    "- **Sistemas de recomendación:** Sugerir productos a un usuario basándose en usuarios similares.\n",
    "- **Diagnóstico médico:** Predecir enfermedades comparando síntomas de un paciente con casos previos.\n",
    "- **Detección de anomalías:** Identificar valores atípicos que no se parecen a sus vecinos.\n",
    "\n",
    "**Ventajas:**  \n",
    "- Sencillo de implementar y entender.\n",
    "- No requiere entrenamiento explícito (modelo perezoso).\n",
    "- Funciona bien con fronteras de decisión no lineales.\n",
    "\n",
    "**Desventajas:**  \n",
    "- Costoso computacionalmente para grandes conjuntos de datos.\n",
    "- Sensible a la escala de las variables y a la elección de $k$.\n",
    "- Puede verse afectado por datos irrelevantes o ruido."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
